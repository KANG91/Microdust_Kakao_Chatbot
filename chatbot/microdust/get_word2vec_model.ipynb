{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 목적 : 미세먼지봇 구현. \n",
    "1. 사용자의 텍스트 입력 ex)상도역, 강남 교보문고\n",
    "2. 구글 API에서 지역 이름 출력(도(광역시), 시, 동(면))\n",
    "3. 동 이름을 이용하여 환경공단 API에 대입, 최단거리의 관측소 좌표 크롤링\n",
    "4. 환경공단 API에서 해당 지역 데이터를 제공하지 못하는경우\n",
    "> 지역이름을 벡터화, 코사인 유사도로 지형적으로 가까운 근처 동 이름 제공\n",
    "> 관측소 좌표 크롤링  \n",
    "\n",
    "5. 환경공단 API에서 해당 지역 데이터를 제공하는 경우\n",
    "> 관측소 좌표 크롤링\n",
    "\n",
    "6. 관측소를 기준으로 측정한 최신 미세먼지 데이터 크롤링\n",
    "7. 사용자에게 return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4번 단계 구현을 위한 word2vec 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  word2vec의 CBOW\n",
    "- \"비슷한 단어는 같은 문맥에 사용된다\"\n",
    "- text 파일 사용, 성능 향상을 위한 특수 기호 및 영어의 경우 stemming 사용\n",
    "- 이후 tokenization 수행, 이 때 tokenization은 string -> list를 의미.  \n",
    "ex) [['token', 'token'], ['token', 'token']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codec이란?\n",
    "- 파이썬에서 일반적으로 유니코드를 다룰 때 사용하는 decode, encode는 대용량 데이터인 경우, 모든 데이터가 메모리에 올라감\n",
    "- ** codecs ** 모듈을 통해 파일을 조금씩 읽어, 메모리에도 약간의 데이터만 올려줄 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주소가 있는 csv 파일들을 불러와 word2vec에 넣기 위한 전처리하는 함수\n",
    "# 큰 단위의 주소와 세부 단위 주소 따로 합쳐져 있는 파일을 동일 value를 가지는 column을 기준으로 merge\n",
    "def merge_and_to_csv(add_fname, sub_fname) : \n",
    "    address_lines = open(\"datas/\" + add_fname + \".txt\", \"r\")\n",
    "    sub_lines = open(\"datas/\" + sub_fname + \".txt\", \"r\")\n",
    "    \n",
    "    address_list, sub_list = [], []\n",
    "    \n",
    "    for add_line in address_lines : \n",
    "        add_line = (add_line).split(\"|\")\n",
    "        address_list.append(add_line)\n",
    "    \n",
    "    for sub_line in sub_lines : \n",
    "        sub_line = sub_line.split(\"|\")\n",
    "        sub_list.append(sub_line)\n",
    "        \n",
    "    address_df = pd.DataFrame(address_list)\n",
    "    address_df = address_df[[0, 3, 4, 5]]\n",
    "    \n",
    "    sub_df = pd.DataFrame(sub_list)\n",
    "    sub_df = sub_df[[0, 2, 6, 7]]\n",
    "    \n",
    "    full_address = pd.merge(address_df, sub_df, how = 'left', on = [0])\n",
    "    full_address.to_csv(\"full_\" + add_fname + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#작업 폴더 내 파일들 이름으로 리스트 만들기\n",
    "ls = glob.glob(\"C:/Users/ledes/study/Microdust_Kakao_chatbot/chatbot/microdust/datas/*csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 큰 단위 주소, 세부 단위 주소를 인덱싱해 리스트에 담기.\n",
    "ls = ls[ : -1]\n",
    "file_ls = []\n",
    "for idx in range(len(ls)) : \n",
    "    f_ls = ls[idx][69 :  ]\n",
    "    f_ls = f_ls[ : -4]\n",
    "    file_ls.append(f_ls)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for address, sub in zip(file_ls[:17], file_ls[17:]) :\n",
    "    merge_and_to_csv(address, sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_address_ls = []\n",
    "for name in file_ls[:17] : \n",
    "    full_name = \"full\" + name\n",
    "    full_address_ls.append(full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec model들의 이름만 \n",
    "check_ls = glob.glob(\"C:/Users/ledes/study/Microdust_Kakao_chatbot/chatbot/microdust/model*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_ls = [check_ls[i][69:] for i in range(len(check_ls))]\n",
    "answer_ls = [file_ls[i][13 : -4] for i in range(len(file_ls))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sejong', 'seoul', 'ulsan']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for문 과정에서 발생한 오류 확인 -> 작업이 되지 않은 csv 파일들 내용 확인\n",
    "result = [i for i in answer_ls if i not in check_ls]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ls = glob.glob(\"C:/Users/ledes/study/Microdust_Kakao_chatbot/chatbot/microdust/datas/*csv\")\n",
    "file_ls = [file_ls[i][63: ] for i in range(len(file_ls))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"full_address_sejong.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(labels = [\"0\", \"4\", \"6\", \"7\"], inplace = True, axis = 1, errors = \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_vectors(fname) : \n",
    "    \n",
    "    model_name = \"_\" + fname[13:-4]\n",
    "    print(model_name + \"start\")\n",
    "    #파일 불러오기\n",
    "    df = pd.read_csv(fname)\n",
    "\n",
    "    #필요 데이터 추출\n",
    "    df.drop(labels = [\"0\", \"6\", \"7\"], inplace = True, axis = 1, errors = \"ignore\")\n",
    "\n",
    "    #null 값 제거\n",
    "    df.dropna(axis = 0, how = 'any', inplace = True)\n",
    "\n",
    "    train_data = []\n",
    "    for idx in np.array(df) : \n",
    "        train_data.append(list(idx))\n",
    "\n",
    "    #word2vec parameters\n",
    "    parameters = {\n",
    "        \"sg\" : 1, # skip-gram\n",
    "        \"size\" : 150, # dimesionality of the feature vectors\n",
    "        \"window\" : 3,\n",
    "        \"alpha\" : 0.01, #learning rate\n",
    "        \"batch_words\" : 10000, #사전 구축 시 한 번에 몇 개의 단어를 읽을 것인가\n",
    "        \"iter\" : 10, # epoch과 유사\n",
    "        \"workers\" : multiprocessing.cpu_count(), #cpu\n",
    "\n",
    "    }\n",
    "\n",
    "    #인수 입력\n",
    "    model = Word2Vec(**parameters)\n",
    "\n",
    "    #단어 사전 생성\n",
    "    model.build_vocab(train_data)\n",
    "\n",
    "    #벡터 train\n",
    "    model.train(ttest, total_examples = 100, epochs = 100 )\n",
    "    \n",
    "    model.save('model' + model_name)\n",
    "    \n",
    "    print(model_name + \"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_chungbukstart\n",
      "_chungbukend\n",
      "_chungnamstart\n",
      "_chungnamend\n",
      "_daegustart\n",
      "_daeguend\n",
      "_daejeonstart\n",
      "_daejeonend\n",
      "_gangwonstart\n",
      "_gangwonend\n",
      "_gwangjustart\n",
      "_gwangjuend\n",
      "_gyungbukstart\n",
      "_gyungbukend\n",
      "_gyunggistart\n",
      "_gyunggiend\n",
      "_gyungnamstart\n",
      "_gyungnamend\n",
      "_incheonstart\n",
      "_incheonend\n",
      "_jejustart\n",
      "_jejuend\n",
      "_junbukstart\n",
      "_junbukend\n",
      "_junnamstart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ledes\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2802: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_junnamend\n",
      "_sejongstart\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-8763a1504fba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_ls\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain_word_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-71-050eeb093cff>\u001b[0m in \u001b[0;36mtrain_word_vectors\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m#벡터 train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mttest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[0;32m    609\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[0;32m    567\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[1;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# should be set by `build_vocab`\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "for fname in file_ls : \n",
    "    train_word_vectors(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('사당동', 0.9160662889480591),\n",
       " ('본동', 0.9099898338317871),\n",
       " ('신대방동', 0.892453670501709),\n",
       " ('상도1동', 0.8730696439743042),\n",
       " ('신대방제1동', 0.8628720641136169),\n",
       " ('신대방제2동', 0.8610486388206482),\n",
       " ('노량진제1동', 0.8286268711090088),\n",
       " ('노량진동', 0.8191577196121216),\n",
       " ('노량진제2동', 0.798297643661499),\n",
       " ('상도제1동', 0.7953025698661804)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"동작동\", topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model' + '_seoul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv.similarity(\"흑석동\", \"도봉구\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive = [\"흑석동\", \"종로구\"], negative = [\"상도동\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment : \n",
    " word2vec 실습 결과 깨달은 사실\n",
    "- word2vec 기본은 center word를 바탕으로 주변 context를 판단하거나(skip-gram), 주변 context를 바탕으로 center word를 판단하는(CBOW) 방식이 있음\n",
    "- 구현해야 하는 기능은 **오타가 없다는 전제 하에, API가 인식하지 못하는 지역명을 바로 잡아주는 것(예를 들어, 충북, 구로5동 등)**\n",
    "- 해결 아이디어 : 전국 시,도 등 주소가 적힌 csv파일을 찾아 훈련시켜볼 것\n",
    "\n",
    "갑자기 생각난 아이디어!  \n",
    "- 오타가 없다는 전제 하에 API가 인식하지 못하는 지역명 바로잡기\n",
    "> 전국 주소를 word2vec으로 학습\n",
    "- 이외 데이터 학습은 seq2seq로"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
