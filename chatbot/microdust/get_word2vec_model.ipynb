{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 목적 : 미세먼지봇 구현. \n",
    "1. 사용자의 텍스트 입력 ex)상도역, 강남 교보문고\n",
    "2. 구글 API에서 지역 이름 출력(도(광역시), 시, 동(면))\n",
    "3. 동 이름을 이용하여 환경공단 API에 대입, 최단거리의 관측소 좌표 크롤링\n",
    "4. 환경공단 API에서 해당 지역 데이터를 제공하지 못하는경우\n",
    "> 지역이름을 벡터화, 코사인 유사도로 지형적으로 가까운 근처 동 이름 제공\n",
    "> 관측소 좌표 크롤링  \n",
    "\n",
    "5. 환경공단 API에서 해당 지역 데이터를 제공하는 경우\n",
    "> 관측소 좌표 크롤링\n",
    "\n",
    "6. 관측소를 기준으로 측정한 최신 미세먼지 데이터 크롤링\n",
    "7. 사용자에게 return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4번 단계 구현을 위한 word2vec 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  word2vec의 CBOW\n",
    "- \"비슷한 단어는 같은 문맥에 사용된다\"\n",
    "- text 파일 사용, 성능 향상을 위한 특수 기호 및 영어의 경우 stemming 사용\n",
    "- 이후 tokenization 수행, 이 때 tokenization은 string -> list를 의미.  \n",
    "ex) [['token', 'token'], ['token', 'token']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codec이란?\n",
    "- 파이썬에서 일반적으로 유니코드를 다룰 때 사용하는 decode, encode는 대용량 데이터인 경우, 모든 데이터가 메모리에 올라감\n",
    "- ** codecs ** 모듈을 통해 파일을 조금씩 읽어, 메모리에도 약간의 데이터만 올려줄 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ledes\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 주소가 있는 csv 파일들을 불러와 word2vec에 넣기 위한 전처리하는 함수\n",
    "# 큰 단위의 주소와 세부 단위 주소 따로 합쳐져 있는 파일을 동일 value를 가지는 column을 기준으로 merge\n",
    "def merge_and_to_csv(add_fname, sub_fname) : \n",
    "    address_lines = open(\"datas/\" + add_fname + \".txt\", \"r\")\n",
    "    sub_lines = open(\"datas/\" + sub_fname + \".txt\", \"r\")\n",
    "    \n",
    "    address_list, sub_list = [], []\n",
    "    \n",
    "    for add_line in address_lines : \n",
    "        add_line = (add_line).split(\"|\")\n",
    "        address_list.append(add_line)\n",
    "    \n",
    "    for sub_line in sub_lines : \n",
    "        sub_line = sub_line.split(\"|\")\n",
    "        sub_list.append(sub_line)\n",
    "        \n",
    "    address_df = pd.DataFrame(address_list)\n",
    "    address_df = address_df[[0, 3, 4, 5]]\n",
    "    \n",
    "    sub_df = pd.DataFrame(sub_list)\n",
    "    sub_df = sub_df[[0, 2, 6, 7]]\n",
    "    \n",
    "    full_address = pd.merge(address_df, sub_df, how = 'left', on = [0])\n",
    "    full_address.to_csv(\"full_\" + add_fname + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#작업 폴더 내 파일들 이름으로 리스트 만들기\n",
    "ls = glob.glob(\"C:/Users/ledes/study/Microdust_Kakao_chatbot/chatbot/microdust/datas/*csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 큰 단위 주소, 세부 단위 주소를 인덱싱해 리스트에 담기.\n",
    "ls = ls[ : -1]\n",
    "file_ls = []\n",
    "for idx in range(len(ls)) : \n",
    "    f_ls = ls[idx][69 :  ]\n",
    "    f_ls = f_ls[ : -4]\n",
    "    file_ls.append(f_ls)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for address, sub in zip(file_ls[:17], file_ls[17:]) :\n",
    "    merge_and_to_csv(address, sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_address_ls = []\n",
    "for name in file_ls[:17] : \n",
    "    full_name = \"full\" + name\n",
    "    full_address_ls.append(full_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word2vec model들의 이름만 \n",
    "check_ls = glob.glob(\"C:/Users/ledes/study/Microdust_Kakao_chatbot/chatbot/microdust/model*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_ls = [check_ls[i][69:] for i in range(len(check_ls))]\n",
    "answer_ls = [file_ls[i][13 : -4] for i in range(len(file_ls))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sejong', 'seoul', 'ulsan']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for문 과정에서 발생한 오류 확인 -> 작업이 되지 않은 csv 파일들 내용 확인\n",
    "result = [i for i in answer_ls if i not in check_ls]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_ls = glob.glob(\"C:/Users/ledes/study/Microdust_Kakao_chatbot/chatbot/microdust/*csv\")\n",
    "file_ls = [file_ls[i][63: ] for i in range(len(file_ls))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['full_address_busan.csv',\n",
       " 'full_address_chungbuk.csv',\n",
       " 'full_address_chungnam.csv',\n",
       " 'full_address_daegu.csv',\n",
       " 'full_address_daejeon.csv',\n",
       " 'full_address_gangwon.csv',\n",
       " 'full_address_gwangju.csv',\n",
       " 'full_address_gyungbuk.csv',\n",
       " 'full_address_gyunggi.csv',\n",
       " 'full_address_gyungnam.csv',\n",
       " 'full_address_incheon.csv',\n",
       " 'full_address_jeju.csv',\n",
       " 'full_address_junbuk.csv',\n",
       " 'full_address_junnam.csv',\n",
       " 'full_address_sejong.csv',\n",
       " 'full_address_seoul.csv',\n",
       " 'full_address_ulsan.csv']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_word_vectors(fname) : \n",
    "    \n",
    "    model_name = \"_\" + fname[13:-4]\n",
    "    print(model_name + \" start\")\n",
    "    #파일 불러오기\n",
    "    df = pd.read_csv(fname)\n",
    "\n",
    "    #필요 데이터 추출\n",
    "    # 세종시 데이터가 기존 칼럼과 달라 오류가 발생 -> 세종시만 따로 처리\n",
    "    if \"sejong\" in fname : \n",
    "        test.drop(labels = [\"0\", \"4\", \"6\", \"7\"], inplace = True, axis = 1, errors = \"ignore\")\n",
    "    else : \n",
    "        df.drop(labels = [\"0\", \"6\", \"7\"], inplace = True, axis = 1, errors = \"ignore\")\n",
    "\n",
    "    #null 값 제거\n",
    "    df.dropna(axis = 0, how = 'any', inplace = True)\n",
    "\n",
    "    train_data = []\n",
    "    for idx in np.array(df) : \n",
    "        train_data.append(list(idx))\n",
    "\n",
    "    #word2vec parameters\n",
    "    parameters = {\n",
    "        \"sg\" : 1, # skip-gram\n",
    "        \"size\" : 150, # dimesionality of the feature vectors\n",
    "        \"window\" : 3,\n",
    "        \"alpha\" : 0.01, #learning rate\n",
    "        \"batch_words\" : 10000, #사전 구축 시 한 번에 몇 개의 단어를 읽을 것인가\n",
    "        \"iter\" : 10, # epoch과 유사\n",
    "        \"workers\" : multiprocessing.cpu_count(), #cpu\n",
    "\n",
    "    }\n",
    "\n",
    "    #인수 입력\n",
    "    model = Word2Vec(**parameters)\n",
    "\n",
    "    #단어 사전 생성\n",
    "    model.build_vocab(train_data)\n",
    "\n",
    "    #벡터 train\n",
    "    model.train(train_data, total_examples = model.corpus_count, epochs = model.epochs)\n",
    "    \n",
    "    model.save('model' + model_name)\n",
    "    \n",
    "    print(model_name + \" end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_busan start\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-146-8763a1504fba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_ls\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain_word_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-145-9e197ad3d01f>\u001b[0m in \u001b[0;36mtrain_word_vectors\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m#벡터 train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[0;32m    609\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[0;32m    567\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    255\u001b[0m             trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0;32m    256\u001b[0m                 \u001b[0mdata_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m                 queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[0;32m    258\u001b[0m             \u001b[0mtrained_word_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrained_word_count_epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[0mraw_word_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mraw_word_count_epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[0;32m    225\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0;32m    226\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m             report_delay=report_delay)\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m             \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocks if workers too slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# a thread reporting that it finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fname in file_ls : \n",
    "    train_word_vectors(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_seoul start\n",
      "650633 10\n",
      "_seoul end\n"
     ]
    }
   ],
   "source": [
    "fname = \"full_address_seoul.csv\"\n",
    "model_name = \"_\" + fname[13:-4]\n",
    "print(model_name + \" start\")\n",
    "#파일 불러오기\n",
    "df = pd.read_csv(fname)\n",
    "\n",
    "#필요 데이터 추출\n",
    "# 세종시 데이터가 기존 칼럼과 달라 오류가 발생 -> 세종시만 따로 처리\n",
    "if \"sejong\" in fname : \n",
    "    test.drop(labels = [\"0\", \"4\", \"6\", \"7\"], inplace = True, axis = 1, errors = \"ignore\")\n",
    "else : \n",
    "    df.drop(labels = [\"0\", \"6\", \"7\"], inplace = True, axis = 1, errors = \"ignore\")\n",
    "\n",
    "#null 값 제거\n",
    "df.dropna(axis = 0, how = 'any', inplace = True)\n",
    "\n",
    "train_data = []\n",
    "for idx in np.array(df) : \n",
    "    train_data.append(list(idx))\n",
    "\n",
    "#word2vec parameters\n",
    "parameters = {\n",
    "    \"sg\" : 1, # skip-gram\n",
    "    \"size\" : 150, # dimesionality of the feature vectors\n",
    "    \"window\" : 3,\n",
    "    \"alpha\" : 0.01, #learning rate\n",
    "    \"batch_words\" : 10000, #사전 구축 시 한 번에 몇 개의 단어를 읽을 것인가\n",
    "    \"iter\" : 10, # epoch과 유사\n",
    "    \"workers\" : multiprocessing.cpu_count(), #cpu\n",
    "\n",
    "}\n",
    "\n",
    "#인수 입력\n",
    "model = Word2Vec(**parameters)\n",
    "\n",
    "#단어 사전 생성\n",
    "model.build_vocab(train_data)\n",
    "\n",
    "#벡터 train\n",
    "model.train(train_data, total_examples = model.corpus_count, epochs = 100)\n",
    "\n",
    "\n",
    "print(model_name + \" end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('성수2가제3동', 0.9903359413146973),\n",
       " ('장안동', 0.9883288145065308),\n",
       " ('성수2가제1동', 0.9877767562866211),\n",
       " ('휘경제2동', 0.9877095222473145),\n",
       " ('송정동', 0.9870534539222717),\n",
       " ('답십리제1동', 0.9870078563690186),\n",
       " ('전농제1동', 0.9868310689926147),\n",
       " ('성수1가제1동', 0.9866747856140137),\n",
       " ('장안제2동', 0.9862525463104248),\n",
       " ('장안제1동', 0.9861014485359192)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"회기동\", topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model' + '_seoul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv.similarity(\"흑석동\", \"도봉구\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive = [\"흑석동\", \"종로구\"], negative = [\"상도동\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 서울을 제외한 다른 데이터들의 학습 정도 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ls = glob.glob(\"C:/Users/ledes/study/Microdust_Kakao_chatbot/chatbot/microdust/datas/*txt\")\n",
    "file_ls = [file_ls[idx][69:] for idx in range(len(file_ls))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datas/' + file_ls[8], 'r') as f :\n",
    "    test = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "for idx in range(len(test)) : \n",
    "    line = test[idx].split(\"|\")[3:5+1]\n",
    "    ls.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"sg\" : 1, # skip-gram\n",
    "    \"size\" : 150, # dimesionality of the feature vectors\n",
    "    \"window\" : 3,\n",
    "    \"alpha\" : 0.01, #learning rate\n",
    "    \"batch_words\" : 10000, #사전 구축 시 한 번에 몇 개의 단어를 읽을 것인가\n",
    "    \"iter\" : 10, # epoch과 유사\n",
    "    \"workers\" : multiprocessing.cpu_count(), #cpu\n",
    "\n",
    "}\n",
    "\n",
    "#인수 입력\n",
    "model = Word2Vec(**parameters)\n",
    "\n",
    "#단어 사전 생성\n",
    "model.build_vocab(train_data)\n",
    "\n",
    "#벡터 train\n",
    "model.train(train_data, total_examples = 100, epochs = 100 )\n",
    "\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('대치동', 0.9999366402626038),\n",
       " ('청담동', 0.9997897148132324),\n",
       " ('개포동', 0.9995765686035156),\n",
       " ('논현동', 0.9987701177597046),\n",
       " ('도곡동', 0.9984256625175476),\n",
       " ('역삼동', 0.9982285499572754),\n",
       " ('일원동', 0.9977246522903442),\n",
       " ('자곡동', 0.9930808544158936),\n",
       " ('세곡동', 0.9929624795913696),\n",
       " ('율현동', 0.9924757480621338)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"삼성동\", topn = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
